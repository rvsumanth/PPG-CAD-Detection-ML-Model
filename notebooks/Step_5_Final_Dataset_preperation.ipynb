{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "Q75cjSHYuaot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_patient_centric(input_path, output_path=None):\n",
        "    \"\"\"\n",
        "    Transform medical dataset to patient-centric format for CAD detection model.\n",
        "    Saves output to Google Drive.\n",
        "\n",
        "    Args:\n",
        "        input_path: Path to input CSV file\n",
        "        output_path: Path for output CSV file (default: '/content/drive/MyDrive/processed_patient_data.csv')\n",
        "    \"\"\"\n",
        "\n",
        "    # Mount Google Drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ“ Google Drive mounted successfully\")\n",
        "\n",
        "    # Set default output path if not provided\n",
        "    if output_path is None:\n",
        "        output_path = '/content/drive/MyDrive/processed_patient_data.csv'\n",
        "\n",
        "    # Load the dataset\n",
        "    print(f\"\\nLoading data from {input_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(input_path)\n",
        "        print(f\"âœ“ Data loaded successfully\")\n",
        "        print(f\"Initial dataset shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {input_path}\")\n",
        "        return None\n",
        "\n",
        "    # 1. Remove duplicate rows based on SUBJECT_ID\n",
        "    df_clean = df.drop_duplicates(subset=['SUBJECT_ID'], keep='first')\n",
        "    print(f\"After removing duplicates: {df_clean.shape}\")\n",
        "    print(f\"Removed {len(df) - len(df_clean)} duplicate patient records\")\n",
        "\n",
        "    # 2. Convert GENDER to numeric (M=1, F=0)\n",
        "    gender_mapping = {'M': 1, 'F': 0, 'Male': 1, 'Female': 0, 'male': 1, 'female': 0}\n",
        "    df_clean['GENDER'] = df_clean['GENDER'].map(gender_mapping)\n",
        "\n",
        "    # 3. Handle missing values for Age and Gender\n",
        "    # Fill missing Age with median\n",
        "    if df_clean['AGE'].isnull().any():\n",
        "        age_median = df_clean['AGE'].median()\n",
        "        df_clean['AGE'] = df_clean['AGE'].fillna(age_median)\n",
        "        print(f\"Filled {df_clean['AGE'].isnull().sum()} missing Age values with median: {age_median:.1f}\")\n",
        "\n",
        "    # Fill missing Gender with mode (most frequent)\n",
        "    if df_clean['GENDER'].isnull().any():\n",
        "        gender_mode = df_clean['GENDER'].mode()[0]\n",
        "        df_clean['GENDER'] = df_clean['GENDER'].fillna(gender_mode)\n",
        "        print(f\"Filled {df_clean['GENDER'].isnull().sum()} missing Gender values with mode: {gender_mode}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "FzSZkxX3ubln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 4. Extract patient-known ICD-9 conditions\n",
        "    def extract_icd9_flags(icd9_string):\n",
        "        \"\"\"\n",
        "        Extract binary flags for patient-known conditions from ICD9_CODES string.\n",
        "        \"\"\"\n",
        "        if pd.isnull(icd9_string) or str(icd9_string).strip() == '':\n",
        "            return {'diabetes': 0, 'cholesterol': 0, 'obesity': 0}\n",
        "\n",
        "        codes = str(icd9_string).split(';')\n",
        "\n",
        "        # Initialize flags\n",
        "        flags = {\n",
        "            'diabetes': 0,\n",
        "            'cholesterol': 0,\n",
        "            'obesity': 0\n",
        "        }\n",
        "\n",
        "        # Check each code\n",
        "        for code in codes:\n",
        "            code = code.strip()\n",
        "            # Skip empty codes\n",
        "            if not code:\n",
        "                continue\n",
        "            # Diabetes (codes starting with 250)\n",
        "            if code.startswith('250'):\n",
        "                flags['diabetes'] = 1\n",
        "            # Cholesterol/Hyperlipidemia (codes starting with 272)\n",
        "            elif code.startswith('272'):\n",
        "                flags['cholesterol'] = 1\n",
        "            # Obesity (codes starting with 278)\n",
        "            elif code.startswith('278'):\n",
        "                flags['obesity'] = 1\n",
        "\n",
        "        return flags\n",
        "\n",
        "    # Apply the extraction function\n",
        "    icd9_flags = df_clean['ICD9_CODES'].apply(extract_icd9_flags)\n",
        "\n",
        "    # Create new columns from the extracted flags\n",
        "    df_clean['is_diabetic'] = [flags['diabetes'] for flags in icd9_flags]\n",
        "    df_clean['has_high_cholesterol'] = [flags['cholesterol'] for flags in icd9_flags]\n",
        "    df_clean['is_obese'] = [flags['obesity'] for flags in icd9_flags]"
      ],
      "metadata": {
        "id": "1XUEn9UEupAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 5. Create CAD_LABEL (1 if codes start with 414 or 410, otherwise 0)\n",
        "    def create_cad_label(icd9_string):\n",
        "        \"\"\"\n",
        "        Create CAD label based on ICD9 codes starting with 414 or 410.\n",
        "        \"\"\"\n",
        "        if pd.isnull(icd9_string) or str(icd9_string).strip() == '':\n",
        "            return 0\n",
        "\n",
        "        codes = str(icd9_string).split(';')\n",
        "\n",
        "        for code in codes:\n",
        "            code = code.strip()\n",
        "            if code.startswith('414') or code.startswith('410'):\n",
        "                return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    df_clean['CAD_LABEL'] = df_clean['ICD9_CODES'].apply(create_cad_label)"
      ],
      "metadata": {
        "id": "s-gCC5Dju1ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 6. Select and order the required columns\n",
        "    # Define PPG columns (check which ones exist in the dataset)\n",
        "    ppg_prefixes = ['ppg_min_']\n",
        "    available_ppg = [col for col in df_clean.columns if any(col.startswith(prefix) for prefix in ppg_prefixes)]\n",
        "\n",
        "    if available_ppg:\n",
        "        print(f\"Found {len(available_ppg)} PPG columns: {available_ppg}\")\n",
        "    else:\n",
        "        print(\"Warning: No PPG columns found in the dataset\")\n",
        "        # Try specific columns if generic search doesn't work\n",
        "        specific_ppg = ['ppg_min_3', 'ppg_min_4', 'ppg_min_5', 'ppg_min_6', 'ppg_min_7', 'ppg_min_8']\n",
        "        available_ppg = [col for col in specific_ppg if col in df_clean.columns]\n",
        "\n",
        "    # Define output columns\n",
        "    output_columns = ['SUBJECT_ID', 'AGE', 'GENDER',\n",
        "                      'is_diabetic', 'has_high_cholesterol', 'is_obese'] + \\\n",
        "                     available_ppg + ['CAD_LABEL']\n",
        "\n",
        "    # Create final DataFrame\n",
        "    processed_df = df_clean[output_columns].copy()\n",
        ""
      ],
      "metadata": {
        "id": "2P0L2Lk-u44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 7. Ensure exactly 1,278 unique patient records\n",
        "    target_count = 1278\n",
        "\n",
        "    if len(processed_df) > target_count:\n",
        "        # If we have more than target, take first target_count records\n",
        "        processed_df = processed_df.head(target_count)\n",
        "        print(f\"Dataset truncated to {target_count} records (as requested)\")\n",
        "    elif len(processed_df) < target_count:\n",
        "        print(f\"Dataset has {len(processed_df)} unique records (target: {target_count})\")\n",
        "        print(\"Note: If you need exactly 1278 records, you may need to combine multiple datasets\")"
      ],
      "metadata": {
        "id": "dLAKpdmHu89e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es4uysQet5Pm"
      },
      "outputs": [],
      "source": [
        "\n",
        "    # 8. Save to Google Drive\n",
        "    print(f\"\\nSaving processed data to Google Drive...\")\n",
        "    print(f\"Output path: {output_path}\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created directory: {output_dir}\")\n",
        "\n",
        "    processed_df.to_csv(output_path, index=False)\n",
        "    print(f\"âœ“ Successfully saved to Google Drive\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 9. Print summary statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRANSFORMATION COMPLETE - SUMMARY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Output file: {output_path}\")\n",
        "    print(f\"File size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
        "    print(f\"Final dataset shape: {processed_df.shape}\")\n",
        "    print(f\"Unique patients: {processed_df['SUBJECT_ID'].nunique()}\")\n",
        "\n",
        "    print(\"\\nðŸ“Š DEMOGRAPHICS:\")\n",
        "    print(f\"- Age range: {processed_df['AGE'].min():.1f} - {processed_df['AGE'].max():.1f} years\")\n",
        "    print(f\"- Mean age: {processed_df['AGE'].mean():.1f} Â± {processed_df['AGE'].std():.1f} years\")\n",
        "\n",
        "    gender_counts = processed_df['GENDER'].value_counts()\n",
        "    if len(gender_counts) == 2:\n",
        "        print(f\"- Gender: {gender_counts[1]} Male, {gender_counts[0]} Female\")\n",
        "        print(f\"- Male/Female ratio: {gender_counts[1]/gender_counts[0]:.2f}\")\n",
        "\n",
        "    print(\"\\nðŸ¥ PATIENT-KNOWN CONDITIONS:\")\n",
        "    conditions = ['is_diabetic', 'has_high_cholesterol', 'is_obese']\n",
        "    for condition in conditions:\n",
        "        count = processed_df[condition].sum()\n",
        "        percentage = (count / len(processed_df)) * 100\n",
        "        condition_name = condition.replace('_', ' ').title()\n",
        "        print(f\"- {condition_name}: {count} patients ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nðŸŽ¯ CAD DETECTION LABELS:\")\n",
        "    cad_count = processed_df['CAD_LABEL'].sum()\n",
        "    cad_percentage = (cad_count / len(processed_df)) * 100\n",
        "    print(f\"- CAD Positive: {cad_count} patients ({cad_percentage:.1f}%)\")\n",
        "    print(f\"- CAD Negative: {len(processed_df) - cad_count} patients ({100 - cad_percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nðŸ“ˆ DATA QUALITY CHECK:\")\n",
        "    print(f\"- Missing values in AGE: {processed_df['AGE'].isnull().sum()}\")\n",
        "    print(f\"- Missing values in GENDER: {processed_df['GENDER'].isnull().sum()}\")\n",
        "    print(f\"- Missing values in CAD_LABEL: {processed_df['CAD_LABEL'].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "L3N_V1h2vEdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Preview the data\n",
        "    print(\"\\nðŸ‘ï¸  DATA PREVIEW (first 5 rows):\")\n",
        "    print(processed_df.head())\n",
        "\n",
        "    print(\"\\nðŸ“‹ COLUMNS IN OUTPUT FILE:\")\n",
        "    for i, col in enumerate(processed_df.columns, 1):\n",
        "        print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "    return processed_df, output_path\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xyf4pFt3vLPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_google_drive_file(file_path):\n",
        "    \"\"\"\n",
        "    Verify that the file was saved correctly to Google Drive.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GOOGLE DRIVE VERIFICATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(file_path):\n",
        "            df_check = pd.read_csv(file_path)\n",
        "            print(f\"âœ“ File exists at: {file_path}\")\n",
        "            print(f\"âœ“ File contains {len(df_check)} rows and {len(df_check.columns)} columns\")\n",
        "            print(f\"âœ“ First few rows loaded successfully\")\n",
        "            print(\"\\nSample data from saved file:\")\n",
        "            print(df_check.head(3))\n",
        "\n",
        "            # Check critical columns\n",
        "            required_cols = ['SUBJECT_ID', 'AGE', 'GENDER', 'CAD_LABEL']\n",
        "            missing = [col for col in required_cols if col not in df_check.columns]\n",
        "            if missing:\n",
        "                print(f\"\\nâš ï¸  Warning: Missing required columns: {missing}\")\n",
        "            else:\n",
        "                print(f\"\\nâœ… All critical columns present\")\n",
        "\n",
        "        else:\n",
        "            print(f\"âœ— File not found at: {file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error verifying file: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "anaReaHwvnUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function for Google Colab\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run in Google Colab environment.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    choice = input(\"\\nEnter '1', '2', or '3': \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        # File in Google Drive\n",
        "        input_path = input(\"Enter full Google Drive path to your CSV file: \").strip()\n",
        "    elif choice == '2':\n",
        "        # File uploaded to Colab\n",
        "        input_path = input(\"Enter path to your uploaded CSV file: \").strip()\n",
        "    else:\n",
        "        print(\"Invalid choice. Using default sample data.\")\n",
        "        return\n",
        "\n",
        "        # Ask for output file name\n",
        "    output_name = input(\"\\nEnter output file name (or press Enter for default): \").strip()\n",
        "    if not output_name:\n",
        "        output_path = '/content/drive/MyDrive/processed_patient_data.csv'\n",
        "    else:\n",
        "        if not output_name.endswith('.csv'):\n",
        "            output_name += '.csv'\n",
        "        output_path = f'/content/drive/MyDrive/{output_name}'\n",
        "\n",
        "    # Process the data\n",
        "    try:\n",
        "        processed_df, final_output_path = transform_to_patient_centric(input_path, output_path)\n",
        "\n",
        "        # Verify the saved file\n",
        "        verify_google_drive_file(final_output_path)\n",
        "        print(\"âœ… TRANSFORMATION SUCCESSFUL!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error during processing: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "farQFInfv4Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "RcE6cJ0Sv5rj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}